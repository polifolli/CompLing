{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). Используйте метрику BLEU. Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [END] в текущем коде не сработает).\n",
        "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b0ea3c",
      "metadata": {
        "id": "e3b0ea3c"
      },
      "outputs": [],
      "source": [
        "# !apt-get install unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K-ETN3TGoLDg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-ETN3TGoLDg",
        "outputId": "11622d8c-e7f0-41d4-ec2d-3f3ff257158c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JN1Z9HSXoNSK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN1Z9HSXoNSK",
        "outputId": "46c7308c-210b-4fdb-e9e3-3cdf0b7fc2dc"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9220b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9220b9d",
        "outputId": "96c35d9f-ce92-48ca-a5e2-e0484d506722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.1.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import keras\n",
        "import torch\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d650e9eb",
      "metadata": {
        "id": "d650e9eb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# %pip install tokenizers matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "947b3313",
      "metadata": {
        "id": "947b3313"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "import keras\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "kvUX7tdtc5pZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvUX7tdtc5pZ",
        "outputId": "c3bb57b0-9c87-4454-c12d-e1a9d4eb3890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-04 14:52:18--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  85.3MB/s    in 1.4s    \n",
            "\n",
            "2024-04-04 14:52:20 (85.3 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
            "\n",
            "--2024-04-04 14:52:20--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  79.1MB/s    in 0.8s    \n",
            "\n",
            "2024-04-04 14:52:21 (79.1 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
            "\n",
            "--2024-04-04 14:52:21--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305669 (299K)\n",
            "Saving to: ‘opus.en-ru-test.ru’\n",
            "\n",
            "opus.en-ru-test.ru  100%[===================>] 298.50K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-04-04 14:52:21 (3.30 MB/s) - ‘opus.en-ru-test.ru’ saved [305669/305669]\n",
            "\n",
            "--2024-04-04 14:52:21--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173307 (169K)\n",
            "Saving to: ‘opus.en-ru-test.en’\n",
            "\n",
            "opus.en-ru-test.en  100%[===================>] 169.25K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-04 14:52:22 (2.64 MB/s) - ‘opus.en-ru-test.en’ saved [173307/173307]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "38911d06",
      "metadata": {
        "id": "38911d06"
      },
      "outputs": [],
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e110ff04",
      "metadata": {
        "id": "e110ff04"
      },
      "outputs": [],
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b8fae3",
      "metadata": {
        "id": "35b8fae3"
      },
      "source": [
        "Пример перевода с английского на русский"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0eb9b498",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb9b498",
        "outputId": "c1cc2d9c-5947-4ac3-e04b-dcd659f44cc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('so what are you thinking?', 'ну и что ты думаешь?')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sents[-1], ru_sents[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0be060a4",
      "metadata": {
        "id": "0be060a4"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_ru = Tokenizer(WordPiece(), )\n",
        "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_ru = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "xWX3xnMUzbdt",
      "metadata": {
        "id": "xWX3xnMUzbdt"
      },
      "outputs": [],
      "source": [
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_ru.decoder = decoders.WordPiece()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "496d0ea7",
      "metadata": {
        "id": "496d0ea7"
      },
      "outputs": [],
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "# tokenizer_en.save('tokenizer_en')\n",
        "# tokenizer_ru.save('tokenizer_ru')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4661964",
      "metadata": {
        "id": "f4661964"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dc003758",
      "metadata": {
        "id": "dc003758"
      },
      "outputs": [],
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    if target:\n",
        "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
        "                [tokenizer.token_to_id('[END]')]\n",
        "    else:\n",
        "        return tokenizer.encode(text).ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7fc2dae1",
      "metadata": {
        "id": "7fc2dae1"
      },
      "outputs": [],
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, True) for t in ru_sents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5cc0a376",
      "metadata": {
        "id": "5cc0a376"
      },
      "outputs": [],
      "source": [
        "# обратите внимание, что в seq2seq длины могут быть разными\n",
        "max_len_en, max_len_ru = 45, 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f4f31fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f4f31fa",
        "outputId": "0ec0ef98-8a9b-4920-a21f-47d5d8434877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "49ae735e",
      "metadata": {
        "id": "49ae735e"
      },
      "outputs": [],
      "source": [
        "X_en = keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_ru_out = keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post',\n",
        "              value=PAD_IDX)\n",
        "\n",
        "X_ru_dec = keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1,\n",
        "              padding='post', value=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "62d4e3da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "62d4e3da",
        "outputId": "c9803554-de56-405e-bc8d-65d83054c15e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"yeah, that's not exactly...\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "597680b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "597680b6",
        "outputId": "c5ab80a1-0991-4647-9b9c-cc08d019d835"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3280,   13, 2763,    8,   58, 2808, 5148, 2856,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1], dtype=int32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_en[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d751b1d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d751b1d5",
        "outputId": "41471998-1ff7-4573-d8c2-781dc96e2b19"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'да, но не совсем...'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ru_sents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ed95081e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed95081e",
        "outputId": "a7043629-0336-46a4-db77-695ddf2908e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   2, 2588,   15, 2589, 2513, 5362, 2643,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1], dtype=int32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_ru_dec[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "14d7c918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14d7c918",
        "outputId": "a5dae2b0-46d6-4054-8663-827f151e6efe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2588,   15, 2589, 2513, 5362, 2643,    3,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1], dtype=int32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_ru_out[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "824dcb29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "824dcb29",
        "outputId": "d2214648-3b89-49ab-f16b-40f8b7ee0aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1000000, 45), (1000000, 47))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# миллион примеров\n",
        "X_en.shape, X_ru_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "25fa5f05",
      "metadata": {
        "id": "25fa5f05"
      },
      "outputs": [],
      "source": [
        "(X_en_train, X_en_valid,\n",
        "X_ru_dec_train, X_ru_dec_valid,\n",
        "X_ru_out_train, X_ru_out_valid) = train_test_split(X_en,\n",
        "                                                  X_ru_dec,\n",
        "                                                  X_ru_out,\n",
        "                                                  test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "656be820",
      "metadata": {
        "id": "656be820"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
        "    matmul_qk = keras.ops.matmul(query, keras.ops.transpose(key, axes=[0, 1, 3, 2]))\n",
        "\n",
        "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
        "    depth = keras.ops.cast(key.shape[-1], torch.float32)\n",
        "\n",
        "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
        "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
        "    logits = matmul_qk / keras.ops.sqrt(depth)\n",
        "\n",
        "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # Применяем функцию softmax для получения весов внимания\n",
        "    attention_weights = keras.ops.softmax(logits, axis=-1)\n",
        "\n",
        "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
        "    output = keras.ops.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f4b51870",
      "metadata": {
        "id": "f4b51870"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads  # количество голов для внимания\n",
        "        self.d_model = d_model  # размерность вектора модели\n",
        "\n",
        "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
        "\n",
        "        # Создаем полносвязные слои для запроса, ключа и значения\n",
        "        self.query_dense = keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # Создаем последний полносвязный слой\n",
        "        self.dense = keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        # Разделяем входные данные на головы\n",
        "        inputs = keras.ops.reshape(\n",
        "            inputs, newshape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return keras.ops.transpose(inputs, axes=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs.get('mask', None)\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Разделяем запрос, ключ и значение на головы\n",
        "        # то есть просто разрезаем вектора на num_heads частей\n",
        "        # и сравниваем все части между собой\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = keras.ops.transpose(scaled_attention, axes=[0, 2, 1, 3])\n",
        "\n",
        "        # Объединяем головы вместе (склеиваем векторы в один)\n",
        "        concat_attention = keras.ops.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
        "        # Он просто добавляет сложности нашей модели\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19dd941b",
      "metadata": {
        "id": "19dd941b"
      },
      "outputs": [],
      "source": [
        "mha = MultiHeadAttention(d_model=10, num_heads=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7c25cb11",
      "metadata": {
        "id": "7c25cb11"
      },
      "outputs": [],
      "source": [
        "inp = np.random.normal(size=(1, 2, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2fa2e4fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fa2e4fc",
        "outputId": "8bfeb389-8efc-40e5-8956-2cb38e025629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[ 0.97277629, -0.10061499,  1.28651324],\n",
              "        [-0.25863553,  0.30786601,  0.94169787]]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7e866e38",
      "metadata": {
        "id": "7e866e38"
      },
      "outputs": [],
      "source": [
        "inputs = {'query': inp,'key': inp, 'value': inp,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d0b7823f",
      "metadata": {
        "id": "d0b7823f"
      },
      "outputs": [],
      "source": [
        "result = mha(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8e5bb366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5bb366",
        "outputId": "27b5c9d7-befd-4603-e277-713e7dc87348"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 10])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0cdee43b",
      "metadata": {
        "id": "0cdee43b"
      },
      "outputs": [],
      "source": [
        "# преобразуем эмбединг в q,k,v вектора\n",
        "q = mha.query_dense(inputs['query'])\n",
        "k = mha.query_dense(inputs['key'])\n",
        "v = mha.query_dense(inputs['value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "33465f95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33465f95",
        "outputId": "5c53cfe4-c626-48e2-a4b3-6753a5a06f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 10])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# размерность не изменилась так как мы еще не порезали на головы\n",
        "q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1a818c70",
      "metadata": {
        "id": "1a818c70"
      },
      "outputs": [],
      "source": [
        "# разрезаем каждый вектор на куски (головы)\n",
        "qh = mha.split_heads(q, q.shape[0])\n",
        "kh = mha.split_heads(k, k.shape[0])\n",
        "vh = mha.split_heads(v, v.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1c182c26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c182c26",
        "outputId": "1b1589e2-11a7-46af-89e6-1f1363c23b80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 2, 5])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# мы указали num_heads 2 - поэтому размерность каждого куска 5 (10 разделить на два)\n",
        "qh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6cbb9578",
      "metadata": {
        "id": "6cbb9578"
      },
      "outputs": [],
      "source": [
        "# keras.ops.transpose(kh).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a7f9d59a",
      "metadata": {
        "id": "a7f9d59a"
      },
      "outputs": [],
      "source": [
        "matmul_qk = keras.ops.matmul(qh, keras.ops.transpose(kh, axes=[0, 1, 3, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5a2ef8d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2ef8d5",
        "outputId": "4f17cbe4-0b0d-4bbe-88df-bace2245b250"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 2, 2])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# На выходе получится тензор вот такой размерности\n",
        "# 5 изменилось на 2\n",
        "# так как для каждого куска у нас по две близости (до каждого слова включая себя)\n",
        "matmul_qk.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "85a6d549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85a6d549",
        "outputId": "1f86dacf-a763-4e93-9a84-71ead4eaf4c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[0.62, 0.38],\n",
              "         [0.38, 0.62]],\n",
              "\n",
              "        [[0.84, 0.16],\n",
              "         [0.5 , 0.5 ]]]], dtype=float32)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras.ops.softmax(matmul_qk, axis=-1).detach().cpu().numpy().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5c48cea2",
      "metadata": {
        "id": "5c48cea2"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = keras.ops.cast(keras.ops.equal(x, PAD_IDX), torch.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, None, None, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "64790293",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64790293",
        "outputId": "0568ce1e-55f5-4a8d-aa40-4d78b38c59b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[1., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_padding_mask([[1,2,3]]) # 1 подставилась для индекса который равен PAD_IDX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "21c0c899",
      "metadata": {
        "id": "21c0c899"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    # эта функция немножко сложная, но суть у нее достаточно простая\n",
        "    # нужно создать треугольную маску, с помощью которой мы закроем\n",
        "    # для каждого токена все последующие токены\n",
        "    seq_len = x.shape[1]\n",
        "    ones_mask = keras.ops.ones((1, seq_len, seq_len), dtype=\"int32\")\n",
        "    row_index = keras.ops.cumsum(ones_mask, axis=-2)\n",
        "    col_index = keras.ops.cumsum(ones_mask, axis=-1)\n",
        "    look_ahead_mask = ~ keras.ops.greater_equal(row_index, col_index)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return keras.ops.maximum(look_ahead_mask, padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b6f988e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f988e6",
        "outputId": "1c5b6090-3d52-4071-a518-2966578e3c0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[False,  True,  True],\n",
              "         [False, False,  True],\n",
              "         [False, False, False]]], device='cuda:0')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# для декодера нам нужно замаскировать следующие токены\n",
        "# так как мы пытаемся их сгенерировать\n",
        "# для этого создается вот такая маска\n",
        "ones_mask = keras.ops.ones((1, 3, 3), dtype=\"int32\")\n",
        "row_index = keras.ops.cumsum(ones_mask, axis=-2)\n",
        "col_index = keras.ops.cumsum(ones_mask, axis=-1)\n",
        "mask = ~ keras.ops.greater_equal(row_index, col_index)\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "499d2f3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "499d2f3d",
        "outputId": "cb54362f-6991-4bef-a382-21a4c79408ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[   0, -100, -100],\n",
              "         [   0,    0, -100],\n",
              "         [   0,    0,    0]]], device='cuda:0')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# в слое с вниманием эта маска будет использоваться чтобы занулить близости с токенами из будущего\n",
        "(mask * -100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9c0f62e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0f62e3",
        "outputId": "2124f538-3d7a-4cb9-882c-d57d37067daf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 1.],\n",
              "          [1., 0., 1.],\n",
              "          [1., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# эта функция также проверяет и паддинг и если он есть то и его тоже замаскирует\n",
        "# 1 - это паддинг айди\n",
        "create_look_ahead_mask(torch.LongTensor([[1,3,2]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "7a1f209e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1f209e",
        "outputId": "06107899-2239-4264-c76b-a6391d815890"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[0., 1., 1.],\n",
              "          [0., 0., 1.],\n",
              "          [0., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_look_ahead_mask(torch.Tensor([[2,4,3]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "41493f7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41493f7e",
        "outputId": "1e9a5178-c4b9-401b-c50e-ba1062cb11f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[0., 1., 1.],\n",
              "          [0., 0., 1.],\n",
              "          [0., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_look_ahead_mask(torch.Tensor([[2,4,3]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e120bbe5",
      "metadata": {
        "id": "e120bbe5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / keras.ops.power(10000, (2 * (i // 2)) / d_model)\n",
        "        return keras.ops.multiply(position, angles)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=torch.arange(0, position, dtype=torch.float32)[:, None],\n",
        "            i=torch.arange(0, d_model, dtype=torch.float32)[None, :],\n",
        "            d_model=d_model)\n",
        "        sines = keras.ops.sin(angle_rads[:, 0::2])\n",
        "        cosines = keras.ops.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = keras.ops.concatenate([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[None, ...]\n",
        "        return keras.ops.cast(pos_encoding, torch.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :inputs.shape[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "68472627",
      "metadata": {
        "id": "68472627"
      },
      "outputs": [],
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    #call_mha\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "89dcc42e",
      "metadata": {
        "id": "89dcc42e"
      },
      "outputs": [],
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = keras.Input(shape=(max_len,), name=\"inputs\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= keras.ops.sqrt(keras.ops.cast(d_model, torch.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    #inputs (они тут называются outputs но это просто такой нейминг,\n",
        "    # этот параметр передается в encoder_layer первым и encoder_layer будет считать его inputs)\n",
        "    # outputs он тут называется для удобства, так как он будет перезаписываться\n",
        "    # чтобы на вход следующему блоку подавать уже не эмбединги,\n",
        "    # а то что получится как результат предыдущего блока\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "a2f90e7c",
      "metadata": {
        "id": "a2f90e7c"
      },
      "outputs": [],
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "1c0dd6a2",
      "metadata": {
        "id": "1c0dd6a2"
      },
      "outputs": [],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = keras.Input(shape=(max_len,), name='inputs')\n",
        "    enc_outputs = keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= keras.ops.sqrt(keras.ops.cast(d_model, torch.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "e356741b",
      "metadata": {
        "id": "e356741b"
      },
      "outputs": [],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = keras.Input(shape=(max_len[0],), name=\"inputs\")\n",
        "    dec_inputs = keras.Input(shape=(max_len[1]-1,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "    look_ahead_mask = keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    dec_padding_mask = keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "                          vocab_size=vocab_size[0],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[0],\n",
        "                        )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "                          vocab_size=vocab_size[1],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[1]-1,\n",
        "                        )(inputs=[dec_inputs, enc_outputs,\n",
        "                                  look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "d23ece2d",
      "metadata": {
        "id": "d23ece2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "L  = keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = keras.ops.cast(keras.ops.not_equal(y_true, PAD_IDX), torch.float32)\n",
        "    loss = keras.ops.multiply(loss, mask)\n",
        "\n",
        "    return keras.ops.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "tdrMkJHOrV_Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdrMkJHOrV_Q",
        "outputId": "96b228de-235d-4ec2-d603-88618c462671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30000, 30000)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "542fcc43",
      "metadata": {
        "id": "542fcc43"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# average model\n",
        "# NUM_LAYERS = 6\n",
        "# D_MODEL = 512\n",
        "# NUM_HEADS = 8\n",
        "# UNITS = 2048\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=[max_len_en, max_len_ru])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('model_ruen.weights.h5',\n",
        "                                            monitor='val_loss',\n",
        "                                            verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "439f7e12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "439f7e12",
        "outputId": "298d6f05-4bb5-417a-c020-8c2f74a890bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2673cd",
      "metadata": {
        "id": "1b2673cd"
      },
      "outputs": [],
      "source": [
        "# model.load_weights('model_ruen.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "308a8e81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "308a8e81",
        "outputId": "8222452a-3d6e-4727-bffb-012cd6abe7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.0880 - loss: 1.6289\n",
            "Epoch 1: val_loss improved from inf to 0.98398, saving model to model_ruen.weights.h5\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1655s\u001b[0m 348ms/step - accuracy: 0.0880 - loss: 1.6289 - val_accuracy: 0.1465 - val_loss: 0.9840\n",
            "Epoch 2/3\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.1471 - loss: 0.9739\n",
            "Epoch 2: val_loss improved from 0.98398 to 0.88115, saving model to model_ruen.weights.h5\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1663s\u001b[0m 350ms/step - accuracy: 0.1471 - loss: 0.9739 - val_accuracy: 0.1587 - val_loss: 0.8811\n",
            "Epoch 3/3\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.1583 - loss: 0.8766\n",
            "Epoch 3: val_loss improved from 0.88115 to 0.84232, saving model to model_ruen.weights.h5\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1665s\u001b[0m 350ms/step - accuracy: 0.1583 - loss: 0.8766 - val_accuracy: 0.1639 - val_loss: 0.8423\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x782e59295360>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train,\n",
        "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
        "             batch_size=200,\n",
        "             epochs=3,\n",
        "             callbacks=[checkpoint]\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "63762869",
      "metadata": {
        "id": "63762869"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def translate(text):\n",
        "    input_ids = encode(text, tokenizer_en, target=False)\n",
        "\n",
        "    input_ids = keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32)\n",
        "\n",
        "\n",
        "\n",
        "    output_ids = [tokenizer_ru.token_to_id('[START]') ]\n",
        "\n",
        "    pred = model((input_ids, keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [output_ids], maxlen=max_len_ru-1, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32))).cpu().numpy()\n",
        "\n",
        "\n",
        "    while pred.argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[END]')]:\n",
        "        if len(output_ids) >= max_len_ru:\n",
        "            break\n",
        "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
        "        pred[:, :, tokenizer_ru.token_to_id('[UNK]')] = -100\n",
        "\n",
        "        output_ids.append(pred.argmax(2)[0][-1])\n",
        "        pred = model((input_ids, keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [output_ids], maxlen=max_len_ru-1, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32))).cpu().numpy()\n",
        "\n",
        "    return tokenizer_ru.decode(output_ids[1:], )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "5f16e8b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5f16e8b5",
        "outputId": "0c7fc345-98d8-4609-faae-bb92288f006c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'трансформатора'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"Transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "7d0ee1b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7d0ee1b8",
        "outputId": "91d252e8-eb1c-4ec3-db1b-444922c91c81"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'можно translate thisate??'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"can you translate this sentence?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "vxEmG9_UIpPE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vxEmG9_UIpPE",
        "outputId": "d1c21f03-2197-4de8-8e27-006d0aa75715"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'дороти дорога работает.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"road works ahead - sure hope it does\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "aa0b57d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aa0b57d2",
        "outputId": "cde2d82f-f0e2-4908-e92f-67ffc6f3d08e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'я хочу купить мешок.'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"i want to buy a bag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZPgGsEl8Cul",
      "metadata": {
        "id": "-ZPgGsEl8Cul"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MX9WvnhW_UNV",
      "metadata": {
        "id": "MX9WvnhW_UNV"
      },
      "source": [
        "Реализация bleu есть в nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2735e9d0",
      "metadata": {
        "id": "2735e9d0"
      },
      "outputs": [],
      "source": [
        "# %pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "YbuJQX9S8E0o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbuJQX9S8E0o",
        "outputId": "5fc11894-1046-4cd2-d7a8-d671d7b7a62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4548019047027907\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
        "\n",
        "\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
        "print(BLEUscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "1e75c215",
      "metadata": {
        "id": "1e75c215"
      },
      "outputs": [],
      "source": [
        "text = open('opus.en-ru-test.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-test.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "TbLvCxxlFCn6",
      "metadata": {
        "id": "TbLvCxxlFCn6"
      },
      "outputs": [],
      "source": [
        "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
        "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "Dy0cRSoLFC0k",
      "metadata": {
        "id": "Dy0cRSoLFC0k"
      },
      "outputs": [],
      "source": [
        "#НЕ ТРОГАТЬ ЯЧЕЙКУ И ПЕРЕМЕННУЮ\n",
        "translations = []\n",
        "\n",
        "for i in range(len(en_sents_test)):\n",
        "    translations.append(translate(en_sents_test[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "Cuj_lcoUFg9W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuj_lcoUFg9W",
        "outputId": "37b861ac-de68-4dc5-ac00-e92e8528fa05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['while i was tracking his i.p. address for castle,',\n",
              " \"you're mom's baby.\",\n",
              " 'i would be if your brothers had anything to do with it.',\n",
              " 'the bosnia and herzegovina council of ministers has also recommended to the entity ministries of administration and local administration, cantonal and municipal ministries and services responsible for this area, to ensure an institutional framework, as well as human and financial resources, in order to solve this problem during 2006.',\n",
              " \"you just need to hear it's okay.\",\n",
              " 'i simply went for a drive and had an accident that affected my memory.',\n",
              " \"the reception's at 6 p.m., dinner at 8 p.m.\",\n",
              " 'so i picked him up.',\n",
              " 'problems would arise only if accession by reios affected the rights and obligations of other parties, but he was confident that that would not be the case.',\n",
              " 'the requested url /high_precision_die_cutting_machine_hydraulic_type.html was not found on this server.']"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sents_test[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "pRY7yBESGYxa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRY7yBESGYxa",
        "outputId": "42131a90-57e6-4c19-e7a9-2bab583dba3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['я отследила его ip-адрес по просьбе касла.',\n",
              " 'ты мой сыночек.',\n",
              " 'я мог бы стать им, если бы твои браться вмешались.',\n",
              " 'были организованы практикумы и общественные слушания по следующим вопросам: профилактика и охрана здоровья и защита жертв насилия.',\n",
              " '- тед, ты азартен. - ты хочешь выиграть.',\n",
              " 'я не собирался скрываться, сэр. я катался и попал в аварию, что и повлияло на мою память.',\n",
              " 'прием в 6 часов, ужин в 8!',\n",
              " 'я его снял.',\n",
              " 'проблемы могут возникнуть только в том случае, если присоединение роэи будет затрагивать права и обязанности других сторон, однако он уве-рен, что этого не произойдет.',\n",
              " 'the requested url /pet_bottles_recycling_a_level.html was not found on this server.']"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ru_sents_test[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "GoMosjhNhUrD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoMosjhNhUrD",
        "outputId": "e8f6e4d3-ab50-4140-88da-d2cc443d2641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['пока я отслеживал егочу, я.',\n",
              " 'ты ребенок мамы.',\n",
              " 'я буду делать что с с. если у твои',\n",
              " 'в 2006 году в. в году в г. г. г. г. г. г. г. г. г. г г. г г г г н. центр по',\n",
              " 'тебе просто нужно слышать.',\n",
              " 'я просто пришла на и в авариююююююююююююююю.',\n",
              " 'прием в,, вееее в часов вечера. вх..',\n",
              " 'я его под ног.',\n",
              " 'проблемы возникают только лишь, если не затро в условиях защиты прав и и других,,, не он был уверен. он уверен, не.',\n",
              " 'the requested url / / / /_08 / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /.']"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translations[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "hx3kUhHQFhMQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx3kUhHQFhMQ",
        "outputId": "62bc1149-efe5-4fd1-d008-466fdd86a306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "    reference = tokenizer_ru.encode(t).tokens\n",
        "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
        "\n",
        "    bleus.append(round(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "rJDX-Z-3GKDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDX-Z-3GKDE",
        "outputId": "dc6e6cd6-d9d2-4e03-f468-1fba4e97d84f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.5727499999999996"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "hS1K2TjjhjuL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS1K2TjjhjuL",
        "outputId": "de3099bd-b867-4244-a38b-7a4b7d14c510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1: Index = 62, Value = 1.0\n",
            "Top 2: Index = 138, Value = 1.0\n",
            "Top 3: Index = 404, Value = 1.0\n",
            "Top 4: Index = 702, Value = 1.0\n",
            "Top 5: Index = 980, Value = 1.0\n",
            "Top 6: Index = 1260, Value = 1.0\n",
            "Top 7: Index = 1460, Value = 1.0\n",
            "Top 8: Index = 1650, Value = 1.0\n",
            "Top 9: Index = 1353, Value = 0.819\n",
            "Top 10: Index = 1869, Value = 0.809\n",
            "Top 11: Index = 358, Value = 0.779\n",
            "Top 12: Index = 1359, Value = 0.76\n",
            "Top 13: Index = 260, Value = 0.717\n",
            "Top 14: Index = 1206, Value = 0.717\n",
            "Top 15: Index = 1432, Value = 0.717\n"
          ]
        }
      ],
      "source": [
        "indexed_list = list(enumerate(bleus))\n",
        "sorted_list = sorted(indexed_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "top_15_indices = [index for index, _ in sorted_list[:15]]\n",
        "top_15_values = [value for _, value in sorted_list[:15]]\n",
        "\n",
        "for i, (index, value) in enumerate(zip(top_15_indices, top_15_values), 1):\n",
        "    print(f\"Top {i}: Index = {index}, Value = {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "rX2EabADKepo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX2EabADKepo",
        "outputId": "9a833b15-8bbf-491f-8208-ba5c2e4be722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12844 - 12844\n",
            "siberian federal university - сибирский федеральный университет\n",
            "s5000 - s5000\n",
            "98799 - 98799\n",
            "he was ordered to find a solution. - он приказал найти решение.\n",
            "that's not the uterus. - это не матка.\n",
            "jon: what is this? - что это?\n",
            "320757 - 320757\n",
            "- then tell me the truth - - - тогда скажи правду.\n",
            "agenda item 109 (a) - пункт 109 ( a ) повестки повестки\n",
            "our friend over in brussels? - наш друг в брюсселе?\n",
            "4,907usd - 4, 907us\n",
            "- yeah. - - - да...\n",
            "i'm almost back. - я почти вернулся.\n",
            "i don't know. - я не знаю.\n"
          ]
        }
      ],
      "source": [
        "for index in top_15_indices:\n",
        "    original_sentence = en_sents_test[index]\n",
        "    translated_sentence = translations[index]\n",
        "    print(f'{original_sentence} - {translated_sentence}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mEhpY1vJMCeq",
      "metadata": {
        "id": "mEhpY1vJMCeq"
      },
      "source": [
        "Хорошо переводятся числа (хотя их не сложно переводить😇) и короткие фразы"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
        "\n",
        "Ответ должен содержать как минимум 10 предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bhn_CYAx8QsP",
      "metadata": {
        "id": "bhn_CYAx8QsP"
      },
      "source": [
        "## Ответ\n",
        "backtranslation - распространенная техника аугментации данных при машинном переводе. Ее применяют, когда в налчиии нет достаточно большого параллельного корпуса и поэтмоу приходится его создавать самостоятельно. При этом для того, чтобы применить такую технику необходим большой корпус на языке-таргете. По сути, мы пользуемся тем, что имеем, чтобы синтетически дополнить параллеьный корпус. В процессе выполняются следующие шаги:  \n",
        "\n",
        "\n",
        "1.   Ставим задачу перевода с языка Source на язык Target (в нашем случае S=en, T=ru)\n",
        "2.   Если у нас маленький параллельный корпус (en-ru), то появляется повод применить технику backtranslation\n",
        "3.   Находим достаточно объемный корпус на языке Target (на русском).\n",
        "4.   Обучаем модель на имеющемся параллельном корпусе, но учим модель в обратную сторону, то есть переводитть с Target на Source (с русского на английский)\n",
        "5.   Так как наша модель умеет переводить с Target на Source, мы можем с ее помощью перевести найденный в пункте 3 корпус на языке Target (то есть мы переведем наш русский корпус на английский). Таким образом мы сами сгенерируем параллельный корпус и сможем добавить его к тому небольшому, что у нас есть.\n",
        "6.   Сгенерированные параллельные тексты можно добавить к training сету и обучить новую модель переводить с языка Source на язык Target (с англ на рус)\n",
        "\n",
        "Таким образом, применяя технику backtranslation к паре en->ru потребуется одна модель для обучения ru->en и одна модель для обучения en->ru.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
