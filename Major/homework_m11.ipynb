{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "IaerYcn0QcKa",
      "metadata": {
        "id": "IaerYcn0QcKa"
      },
      "source": [
        "# Домашнее задание № 11. Генерация текста\n",
        "## Задание 1 (10 баллов).\n",
        "В семинаре мы работали с датасетами инструкций alpaca и dolly. Они англоязычные. В домашке вам нужно создать аналогичный датасет на русском языке и обучить аналогичную модель на этом датасете. В качестве итогового результата у вас должна получится модель, которая может связно отвечать на русскоязычные инструкции на русском языке. Приведите как минимум три разных примера. Правильность ответов не так важна, так как вы скорее всего будете использовать небольшие модели, но текст должен быть не рандомным.\n",
        "\n",
        "Русскоязычный датасет инструкций должен быть больше 5 тысяч примеров. Он может быть основнован на alpaca/dolly (например, вы можете просто прогнать все через переводную модель, которая была на семинаре, или даже google translate). Или вы можете придумать способ создать аналогичный датасет каким-то другим способом (переделать открытые датасеты с помощью правил). Датасет может быть не уникальным, можно скооперироваться с одногруппниками и сделать один датасет на всех.\n",
        "\n",
        "Вы можете попробовать дообучать любую небольшую decoder-only модель. Скорее всего лучше всего будут работать модели, изначально обученные на русском языке (rugpt например). Но возможно даже модели вроде opt можно будет дообучить на русскоязычных инструкциях.\n",
        "\n",
        "Это задание гораздно менее определенное, по сравнению с предыдущими. Поэтому не стесняйтесь задавать дополнительные вопросы в чате или лично, если у вас возникнут трудности."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e77773",
      "metadata": {
        "id": "a6e77773",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# %pip install pandas transformers tokenizers datasets xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980a1211",
      "metadata": {
        "id": "980a1211"
      },
      "source": [
        "Скачаем датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "narTicMTUTwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "narTicMTUTwX",
        "outputId": "a9881334-b675-4c27-c412-c6811122013b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting zstandard\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: zstandard, jsonlines\n",
            "Successfully installed jsonlines-4.0.0 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install zstandard jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3477de2a",
      "metadata": {
        "id": "3477de2a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Polifolli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import json, os\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
        "\n",
        "# import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9xKaDZX2eRdN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xKaDZX2eRdN",
        "outputId": "9f19d13a-3caf-41a3-bf02-344456f5021e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jiG9ran4Zqqx",
      "metadata": {
        "id": "jiG9ran4Zqqx"
      },
      "outputs": [],
      "source": [
        "# для моего первого аккаунта\n",
        "with open('/content/drive/MyDrive/КомпЛинг/ДЗ 11/ru_turbo_alpaca.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bpiUEBDP3W9K",
      "metadata": {
        "id": "bpiUEBDP3W9K"
      },
      "outputs": [],
      "source": [
        "# для моего второго аккаунта\n",
        "with open('/content/drive/MyDrive/КомпЛинг/ru_turbo_alpaca.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e43bf31f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# для моего компа\n",
        "with open('ru_turbo_alpaca.jsonl', encoding = 'utf-8') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "QToAW2m1cEAv",
      "metadata": {
        "id": "QToAW2m1cEAv"
      },
      "outputs": [],
      "source": [
        "attributes_to_keep = [\"instruction\", \"input\", \"output\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5Iqa5PyHcGFm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iqa5PyHcGFm",
        "outputId": "2a41c768-e175-47b2-e0d3-a1857dbf89f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'Опишите, как сделать горшок из глины.',\n",
              "  'input': '',\n",
              "  'output': 'Для изготовления горшка из глины сначала необходимо взять глину и размешать ее водой до состояния пластилина. Затем нужно сформировать глиняный комок нужной формы и тщательно откачать влагу из глины, чтобы она стала твердой. Горшок нужно обжечь в духовке при высокой температуре, чтобы он стал прочным и непроницаемым.'},\n",
              " {'instruction': 'Опиши процесс изготовления торта. Укажи ингредиенты, необходимые для этого.',\n",
              "  'input': '',\n",
              "  'output': 'Для изготовления торта необходимы следующие ингредиенты: мука, сахар, яйца, молоко, сливочное масло, разрыхлитель, ванилин. Сначала нужно смешать муку с сахаром и добавить яйца, молоко, разрыхлитель и ваниль. Затем тесто надо хорошо перемешать и разделить на несколько частей. Каждую часть нужно запекать в духовке до готовности. После этого торт надо собрать, сложив слои пирога вместе с начинкой и украшениями.'},\n",
              " {'instruction': 'Напиши аналог данной пословицы на современный лад: \"Не говори гоп, пока не перепрыгнешь\".',\n",
              "  'input': '',\n",
              "  'output': '\"Не показывай свои способности, пока не будет нужно.\"'},\n",
              " {'instruction': 'Представьте, что вы занимаетесь дизайном главной страницы для нового сайта. Опишите, что вы хотели бы видеть на этой странице.',\n",
              "  'input': '',\n",
              "  'output': 'Я бы хотел, чтобы главная страница была простой, но информативной. На ней должны быть ясно выделены основные разделы и функции сайта, а также действительно привлекательный визуальный дизайн. Я бы использовал яркие цвета и простые шрифты, чтобы все было легко читаемо. Также я бы добавил крупные изображения и логотип компании, чтобы сделать страницу более запоминающейся.'},\n",
              " {'instruction': 'Напиши рассказ о своем детстве и самом смешном инциденте, который произошел с тобой.',\n",
              "  'input': '',\n",
              "  'output': 'В детстве я любил играть с моими друзьями во дворе. Однажды, когда мы играли в футбол, я попросил моего друга бросить мне мяч. Я подбежал к мячу, но не учел скользкость на тротуаре и промахнулся, упал и ударился головой о крыльцо. Мои друзья сначала испугались, но затем начали смеяться, когда я упал на землю и заржал вместе с ними. Это был самый смешной момент в моей жизни, и мы все долго смеялись.'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rus_alpaca = [{k: v for k, v in example.items() if k in attributes_to_keep} for example in data]\n",
        "rus_alpaca[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ec2955e",
      "metadata": {
        "id": "2ec2955e"
      },
      "source": [
        "В нем каждый пример это инструкция, опциональный контекст и ответ.\n",
        "Для модели эти примеры еще оборачиваются в специальный промпт, который говорит модели, что она должна следовать инструкциям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1189c2de",
      "metadata": {
        "id": "1189c2de"
      },
      "outputs": [],
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Ниже приведена инструкция, описывающая задачу, инструкции соответствует input, который приводит дополнительный контекст. \"\n",
        "        \"Напишите ответ, который соответствующим образом выполняет запрос.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Ниже приведена инструкция, описывающая задание. \"\n",
        "        \"Напишите ответ, который соответствующим образом выполняет запрос.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a53688c8",
      "metadata": {
        "id": "a53688c8"
      },
      "source": [
        "Давайте попробуем дообучить модель от facebook - opt (она открытыя и устроена как LLama и GPT - это декодер онли модель)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3ff2ed",
      "metadata": {
        "id": "4f3ff2ed"
      },
      "source": [
        "Далее код взят из гитхаба Alpaca и он на торче, но если поизучать его, то будет видно, что тут происходят те же манипуляции, что мы делали раньше (превращение токенов в индексы и паддинг/урезание последовательностей)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "15e8004c",
      "metadata": {
        "id": "15e8004c"
      },
      "outputs": [],
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "189e9970",
      "metadata": {
        "id": "189e9970"
      },
      "outputs": [],
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a0672d",
      "metadata": {
        "id": "83a0672d"
      },
      "source": [
        "Далее это оборачивается к классы, которые предобрабатывают данные к формату huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "93bf46bf",
      "metadata": {
        "id": "93bf46bf"
      },
      "outputs": [],
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        #list_data_dict = json.load(open(data_path))\n",
        "        list_data_dict = rus_alpaca\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1e2217",
      "metadata": {
        "id": "9b1e2217"
      },
      "source": [
        "Загружаем модель"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7mllQq9d4_H3",
      "metadata": {
        "id": "7mllQq9d4_H3"
      },
      "source": [
        "## Другая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zyAEVxAZ3uUn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyAEVxAZ3uUn",
        "outputId": "a840a938-df2f-471b-8e64-ffb690dbbd72"
      },
      "outputs": [],
      "source": [
        "# model_name = 'facebook/opt-350m'\n",
        "model_name_2 = \"inkoziev/rugpt_chitchat\"\n",
        "model_2 = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_2,\n",
        "        max_length=512,\n",
        "        cache_dir=\"huggingface_cache\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DfwDJs2-4kLZ",
      "metadata": {
        "id": "DfwDJs2-4kLZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_name_2,\n",
        "    cache_dir=\"huggingface_cache\",\n",
        "    model_max_length=512,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "nwZbzGBf5EyR",
      "metadata": {
        "id": "nwZbzGBf5EyR"
      },
      "outputs": [],
      "source": [
        "train_args = transformers.TrainingArguments(learning_rate=1e-5,\n",
        "                 num_train_epochs=1,\n",
        "                 per_device_train_batch_size=2,\n",
        "                 gradient_accumulation_steps=1,\n",
        "                 evaluation_strategy='no',\n",
        "                 weight_decay=0.,\n",
        "                 warmup_ratio=0.03,\n",
        "                 lr_scheduler_type=\"cosine\",\n",
        "                 save_strategy='no',\n",
        "                 logging_steps=1000,\n",
        "                 output_dir=\"opt125_instruct_ft\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "SctudnX25phT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SctudnX25phT",
        "outputId": "8844dbc9-3556-4274-8073-58fd8b58c27b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=\"ru_turbo_alpaca.jsonl\")\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "OSeDD3oU5k3P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSeDD3oU5k3P",
        "outputId": "8f21a192-14b8-4e6e-f9c0-7002e3a9e2c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/users/a.perehodov/miniconda3/envs/env_workspace/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "WARNING:accelerate.utils.other:Detected kernel version 4.19.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(model=model_2,\n",
        "                 tokenizer=tokenizer,\n",
        "                 args=train_args,\n",
        "                 train_dataset=train_dataset,\n",
        "                 eval_dataset=None,\n",
        "                 data_collator=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ujEhN2La5xUf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ujEhN2La5xUf",
        "outputId": "82a769bd-71ac-41dc-effc-f7cb0ff51857"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/14911 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 1001/14911 [03:06<42:54,  5.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2727, 'grad_norm': 4.297895908355713, 'learning_rate': 9.964101131324918e-06, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 2001/14911 [06:11<43:11,  4.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.6903, 'grad_norm': 3.964296579360962, 'learning_rate': 9.718557674181557e-06, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 3001/14911 [09:15<35:31,  5.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5471, 'grad_norm': 7.329446792602539, 'learning_rate': 9.251253830859204e-06, 'epoch': 0.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 4001/14911 [12:19<33:19,  5.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4855, 'grad_norm': 8.290715217590332, 'learning_rate': 8.584151710950187e-06, 'epoch': 0.27}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▎      | 5001/14911 [15:24<31:36,  5.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4181, 'grad_norm': 5.284891128540039, 'learning_rate': 7.748603442360308e-06, 'epoch': 0.34}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 6001/14911 [18:30<27:12,  5.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.3337, 'grad_norm': 3.278264045715332, 'learning_rate': 6.7838776998643896e-06, 'epoch': 0.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 7001/14911 [21:35<22:19,  5.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.3334, 'grad_norm': 3.8660006523132324, 'learning_rate': 5.7353141757975106e-06, 'epoch': 0.47}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▎    | 8001/14911 [24:40<22:14,  5.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.297, 'grad_norm': 3.5309391021728516, 'learning_rate': 4.652192728134617e-06, 'epoch': 0.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 9001/14911 [27:44<17:42,  5.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2744, 'grad_norm': 13.167799949645996, 'learning_rate': 3.5854173506589697e-06, 'epoch': 0.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 10001/14911 [30:49<15:24,  5.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2541, 'grad_norm': 4.489448070526123, 'learning_rate': 2.585123812817587e-06, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 11001/14911 [33:52<11:43,  5.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2591, 'grad_norm': 5.116946220397949, 'learning_rate': 1.6983234041953788e-06, 'epoch': 0.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 12001/14911 [36:56<08:34,  5.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2555, 'grad_norm': 4.507835865020752, 'learning_rate': 9.666935217129774e-07, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 13001/14911 [39:59<05:30,  5.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2252, 'grad_norm': 7.495641708374023, 'learning_rate': 4.2461893641326966e-07, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 14001/14911 [43:03<02:35,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2463, 'grad_norm': 5.431273460388184, 'learning_rate': 9.757579538917539e-08, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14911/14911 [45:52<00:00,  5.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 2752.0575, 'train_samples_per_second': 10.836, 'train_steps_per_second': 5.418, 'train_loss': 2.408617751458131, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14911, training_loss=2.408617751458131, metrics={'train_runtime': 2752.0575, 'train_samples_per_second': 10.836, 'train_steps_per_second': 5.418, 'train_loss': 2.408617751458131, 'epoch': 1.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1884439d",
      "metadata": {
        "id": "1884439d"
      },
      "source": [
        "Сохраним модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "142a7410",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "142a7410",
        "outputId": "f8fbced8-9bbf-4a87-f22d-15721622394b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512}\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model('rugpt_1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bda0c1",
      "metadata": {
        "id": "42bda0c1"
      },
      "source": [
        "И давайте попробуем ее на том же тексте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "44557cd9",
      "metadata": {
        "id": "44557cd9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d9696d65",
      "metadata": {
        "id": "d9696d65"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'rugpt_1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97779964",
      "metadata": {
        "id": "97779964"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "707d718a",
      "metadata": {
        "id": "707d718a"
      },
      "outputs": [],
      "source": [
        "def predict_for_instruction(instruction, text, model):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    prompt = (\"Ниже приведена инструкция, описывающая задачу, инструкции соответствует input, который приводит дополнительный контекст. \"\n",
        "              \"Напишите ответ, который соответствующим образом выполняет запрос.\\n\\n\"\n",
        "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
        "\n",
        "    inputs = tokenizer([prompt],\n",
        "                        return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
        "        num_beams=1,\n",
        "#         temperature=0.4,\n",
        "#         max_length=100,\n",
        "        max_new_tokens=20,\n",
        "#         no_repeat_ngram_size=3,\n",
        "    #     repetition_penalty= 5.0,\n",
        "    #     length_penalty=0.01,\n",
        "    #     early_stopping=True,\n",
        "    #     do_sample=True,\n",
        "    #     top_k=30,\n",
        "    #     top_p=0.8,\n",
        "        early_stopping=True,\n",
        "    #     num_return_sequences=3,\n",
        "        num_return_sequences= 1,\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=False,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
        "    return summaries[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "52ddc97d",
      "metadata": {
        "id": "52ddc97d"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Барсуки, зарывшиеся под железнодорожными путями, остановили движение поездов на севере и юге Нидерландов, что привело к длительным отменам по крайней мере на двух линиях.\n",
        "Во вторник днем все поезда были остановлены на оживленной линии между южными городами Ден Босх и Бокстел после того, как животные зарылись в дамбу, несущую рельсы. Национальная железнодорожная компания сообщила, что линия не будет работать как минимум неделю.\n",
        "Подкоп означает, что \"рельсы могут проседать, и тогда безопасность движения поездов больше не гарантируется\", - говорится в заявлении ProRail, компании, обслуживающей железнодорожную сеть Нидерландов.\n",
        "Ранее в этом месяце барсуки также зарылись под рельсы в районе северной деревни Молкверум в провинции Фрисландия, что вывело линию из эксплуатации до следующего месяца, пока рабочие будут добиваться разрешения на перемещение животных.\n",
        "Барсуки являются охраняемыми животными в Нидерландах, поэтому железнодорожники должны получить разрешение на их перемещение или нарушение их среды обитания, прежде чем начать ремонт.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "747e36f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "747e36f1",
        "outputId": "724b7fed-152a-4361-b551-b6d1a511a3f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/users/a.perehodov/miniconda3/envs/env_workspace/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Барсуки, зарывшиеся под железнодорожными поездами, остановили движение поездов на севере и юге голланд'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Дайте краткое изложение этого текста.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b1ebc5b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b1ebc5b1",
        "outputId": "816e3278-ba9a-4ff1-b174-a259ba5aa007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Барсуки, зарывшиеся под железнодорожными поездами, остановили движение поездов на севере и юге голланд'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Give a very short summary of this text.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "31ad2e32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "31ad2e32",
        "outputId": "591304a4-ff7b-4774-9729-c8932d61a54a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Барсуки, зарывшиеся под железнодорожными поездами, остановили движение на севере и юге голландской'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Напишите заголовок для следующего текста.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e8dfeba4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e8dfeba4",
        "outputId": "b47e7718-9161-435e-d981-e884f21a05e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Барсуки - это животные, которые зарылись под железнодорожными поездами на севере и юге Франции'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"О каком животном этот текст?\"\n",
        "predict_for_instruction(instruction, text, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "44bec8b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "text2 = \"\"\"\n",
        "В Индонезии началось извержение вулкана Руанг.\n",
        "Столб пепла поднялся на высоту трех километров. Также в момент извержения было зафиксировано землетрясение магнитудой 3,2.\n",
        "Из-за извержения более 800 жителей близлежащих деревень были вынуждены эвакуироваться. Сейчас запрещено находиться в радиусе четырех километров от вулкана\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1bed49c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'В Индонезию произошло извержение вулкана Руанг. Столб пепла поднялся на высоту 3 километров.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Дайте краткое изложение этого текста.\"\n",
        "predict_for_instruction(instruction, text2, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f7090979",
      "metadata": {},
      "outputs": [],
      "source": [
        "text3 = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "516a37df",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Собачка, Кошка, Собака, Собака, Собака.\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Придумай пять разных кличек для собак\"\n",
        "predict_for_instruction(instruction, text3, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ea088fcb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Для приготовления лазаньи вам понадобятся: тесто, яйца, лук, чеснок, оливковое масло,'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction = \"Расскажи рецепт лазаньи\"\n",
        "predict_for_instruction(instruction, text3, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
